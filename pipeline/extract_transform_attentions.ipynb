{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    squad_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertConfig, BertTokenizer\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
    "from transformers.trainer_utils import is_main_process\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "\n",
    "model_name_or_path = 'models/bert/'\n",
    "\n",
    "\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "max_query_length = 10\n",
    "threads = 12\n",
    "n_gpu = 1\n",
    "\n",
    "input_dir = \"./data/squad\"\n",
    "output_dir = \"./models/bert/\"\n",
    "model_type=\"bert\"\n",
    "# evaluate = True\n",
    "train_file = \"train-v2.0.json\"\n",
    "version_2_with_negative=True\n",
    "per_gpu_eval_batch_size=16\n",
    "\n",
    "\n",
    "n_best_size=20\n",
    "max_answer_length=30\n",
    "do_lower_case=True\n",
    "verbose_logging=True\n",
    "null_score_diff_threshold=0.0\n",
    "\n",
    "global_attention = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=0,1\n",
    "%set_env CUDA_DEVICE_ORDER=PCI_BUS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rapids/notebooks/host\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipeline related parameters\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 255)) # scaler \n",
    "\n",
    "data_dir='/rapids/notebooks/host/QA_attentions_pickled'\n",
    "representation_dir='/rapids/notebooks/host/QA_attentions_pickled/representations'\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x    \n",
    "    \n",
    "representation_model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
    "representation_model.fc = Identity() # pass through values from second to last layer, bypassing linear classifier\n",
    "\n",
    "device = \"cuda:1\"\n",
    "cuda = torch.device('cuda:1')\n",
    "representation_model.to(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline methods\n",
    "\n",
    "def plot_layer_heads(attention, num_layers=12, num_heads=12):\n",
    "  for i in range(0,11):\n",
    "    p = attention[i]\n",
    "    fig, axis = plt.subplots(1,12, figsize=(20,5), facecolor='w', edgecolor='k')\n",
    "    plt.title(f'layer {i}')\n",
    "    head = 0\n",
    "    for axs, ph in zip(axis.flatten(), p):\n",
    "      heatmap = axs.imshow(ph, cmap='hot')\n",
    "    \n",
    "def scale_examples(examples):\n",
    "    num_examples = len(examples)\n",
    "    scaled_examples = np.empty(shape=(num_examples), dtype=np.ndarray)\n",
    "    for i, example in enumerate(examples):\n",
    "        new_example = np.empty(shape=(12,12), dtype=np.ndarray)\n",
    "        for l, layer in enumerate(example): #12 layers\n",
    "            new_layer = np.array([])\n",
    "            for h, head in enumerate(layer): #12 heads\n",
    "                flat_head_transformed = scaler.fit_transform(head)\n",
    "                new_example[l,h] = flat_head_transformed.reshape(384,384)\n",
    "        scaled_examples[i] = new_example\n",
    "    return scaled_examples\n",
    "\n",
    "# create (1, 3, 384, 384) shape expected by barlow twins model\n",
    "def reshape_example(image):\n",
    "    example_channel = np.expand_dims(image, axis=0)\n",
    "    batch = np.append(example_channel, example_channel, axis=0)\n",
    "    batch = np.append(batch, example_channel, axis=0)\n",
    "    example_3channel = np.expand_dims(batch, axis=0)\n",
    "    return example_3channel\n",
    "\n",
    "def get_representations(attentions):\n",
    "    num_attentions = len(attentions)\n",
    "    barlow_representations = np.zeros((num_attentions), np.object)\n",
    "    for i, example in enumerate(attentions):\n",
    "        reshaped_example = np.zeros((12,12), np.object)\n",
    "        for l, layer in enumerate(example):\n",
    "            for h, head in enumerate(layer):\n",
    "                reshaped_head = torch.from_numpy(reshape_example(head)).to(cuda)\n",
    "                representation_head = representation_model(reshaped_head.float())\n",
    "                reshaped_example[l][h] = representation_head.detach().cpu().numpy()\n",
    "        barlow_representations[i] = reshaped_example\n",
    "\n",
    "    return barlow_representations\n",
    "\n",
    "def flatten_layer_heads(representations_tensor):\n",
    "    print(\"flattening layers/heads ...\")\n",
    "    num_examples =  len(representations_tensor)\n",
    "    flat_array = np.zeros((num_examples * 12 * 12), np.ndarray)\n",
    "    i = 0\n",
    "    for example in representations_tensor:\n",
    "        for layer in example:\n",
    "            for h, head in enumerate(layer):\n",
    "                flat_array[i] = head[0]\n",
    "                i += 1\n",
    "    return flat_array\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    eval_batch_size = 1 * max(1, n_gpu)\n",
    "\n",
    "##  This can be used to get the last remaining examples, since batching cuts off at 130000, a multiple of 5000\n",
    "#     subset = torch.utils.data.Subset(dataset, range(130000, 131944))\n",
    "    subset = dataset\n",
    "\n",
    "    eval_sampler = SequentialSampler(subset)\n",
    "    eval_dataloader = DataLoader(subset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    \n",
    "#     eval_sampler = SequentialSampler(dataset)\n",
    "#     eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "#     logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Num examples = %d\", len(subset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "    all_attentions = []\n",
    "    start_time = timeit.default_timer()\n",
    "    attn_count = 130000\n",
    "\n",
    "    representation_df = pd.DataFrame()\n",
    "    \n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "\n",
    "            feature_indices = batch[3]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        for i, feature_index in enumerate(feature_indices):\n",
    "            eval_feature = features[feature_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            attentions = get_layers(outputs.attentions)\n",
    "\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "            all_results.append(result)\n",
    "\n",
    "#             append_list_as_row('QA_bert_attentions.csv', attentions)\n",
    "            all_attentions.append(attentions)\n",
    "\n",
    "    \n",
    "            attn_count += 1\n",
    "            if attn_count % 250 == 0:\n",
    "\n",
    "                logger.info(\"Scaling attention values to 0-255 ...\")\n",
    "                representations = scale_examples(all_attentions)\n",
    "                logger.info(\"Processing to 2048 value representations through barlow_twins ...\")\n",
    "                representations = get_representations(representations)\n",
    "                logger.info(\"Appending results to array/dataframe ...\")\n",
    "                representations = flatten_layer_heads(representations)\n",
    "\n",
    "                df = pd.DataFrame(representations)\n",
    "                df = pd.DataFrame([pd.Series(x) for x in df[0]])\n",
    "                representation_df = representation_df.append(df, ignore_index=True)\n",
    "\n",
    "                print(f\"--- eval to representation batch {attn_count} ---\")          \n",
    "                all_attentions = []\n",
    "                representations = []\n",
    "                df = pd.DataFrame()\n",
    "            \n",
    "            if attn_count % 5000 == 0 or attn_count == 131944:\n",
    "                logger.info(f\"  Outputting Attention File representation_df_{attn_count} to {representation_dir}\")\n",
    "                representation_df.to_csv(os.path.join(representation_dir, f\"representation_df_{attn_count}.csv\"))\n",
    "                representation_df = pd.DataFrame()\n",
    "\n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
    "\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(attention, num_layers=12, num_heads=12):\n",
    "  layers = np.ndarray((num_heads,num_layers), np.ndarray)\n",
    "  for i, layer in enumerate(attention):\n",
    "    layer = layer.detach().cpu().numpy()[0]\n",
    "    for j, head in enumerate(layer):\n",
    "      layers[i,j] = head\n",
    "  return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from csv import writer\n",
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    # Open file in append mode\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        # Create a writer object from csv module\n",
    "        csv_writer = writer(write_obj)\n",
    "        # Add contents of list as last row in the csv file\n",
    "        csv_writer.writerow(list_of_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
    "    # Load data features from cache or dataset file\n",
    "#     input_dir = data_dir else \".\"\n",
    "\n",
    "    cached_features_file = os.path.join(\n",
    "        input_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"train\",\n",
    "            list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(max_seq_length),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    logger.info(\"cached features file: %s\", cached_features_file)\n",
    "#     Init features and dataset from cache if it exists\n",
    "    if os.path.exists(cached_features_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features_and_dataset = torch.load(cached_features_file)\n",
    "        features, dataset, examples = (\n",
    "            features_and_dataset[\"features\"],\n",
    "            features_and_dataset[\"dataset\"],\n",
    "            features_and_dataset[\"examples\"],\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
    "\n",
    "        examples = processor.get_train_examples(input_dir, filename=train_file)\n",
    "\n",
    "        logger.info(\"Got features from dataset file at %s\", input_dir)\n",
    "\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=threads,\n",
    "        )\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/17/2021 04:44:51 - WARNING - __main__ -   device: cuda:1, n_gpu: 1\n",
      "[INFO|configuration_utils.py:528] 2021-07-17 04:44:52,001 >> loading configuration file models/bert/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-17 04:44:52,003 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-17 04:44:52,005 >> Didn't find file models/bert/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1651] 2021-07-17 04:44:52,006 >> Didn't find file models/bert/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-17 04:44:52,007 >> loading file models/bert/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-17 04:44:52,007 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-17 04:44:52,008 >> loading file models/bert/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-17 04:44:52,008 >> loading file models/bert/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1715] 2021-07-17 04:44:52,009 >> loading file None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.warning(\n",
    "    \"device: %s, n_gpu: %s\",\n",
    "    device,\n",
    "    n_gpu\n",
    ")\n",
    "# Set the verbosity to info of the Transformers logger (on main process only):\n",
    "# if is_main_process(args.local_rank):\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config = BertConfig.from_pretrained(model_name_or_path, output_attentions=True) # no config_path?\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    do_lower_case=True,\n",
    "    cache_dir=input_dir,\n",
    "    use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:528] 2021-07-17 04:44:52,126 >> loading configuration file models/bert/config.json\n",
      "[INFO|configuration_utils.py:566] 2021-07-17 04:44:52,127 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1159] 2021-07-17 04:44:52,127 >> loading weights file models/bert/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1345] 2021-07-17 04:44:53,744 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
      "\n",
      "[INFO|modeling_utils.py:1354] 2021-07-17 04:44:53,745 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at models/bert/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
      "07/17/2021 04:44:53 - INFO - __main__ -   cached features file: ./data/squad/cached_train_bert_384\n",
      "07/17/2021 04:44:53 - INFO - __main__ -   Loading features from cached file ./data/squad/cached_train_bert_384\n",
      "07/17/2021 04:46:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/17/2021 04:46:38 - INFO - __main__ -     Num examples = 1944\n",
      "07/17/2021 04:46:38 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating:  13%|█▎        | 249/1944 [00:13<01:32, 18.39it/s]07/17/2021 04:46:51 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  13%|█▎        | 249/1944 [00:29<01:32, 18.39it/s]07/17/2021 04:47:24 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "07/17/2021 04:54:24 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  13%|█▎        | 254/1944 [07:56<19:06:11, 40.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 130250 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|██▌       | 499/1944 [08:02<00:33, 43.03it/s]   07/17/2021 04:54:40 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  26%|██▌       | 499/1944 [08:20<00:33, 43.03it/s]07/17/2021 04:55:06 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "07/17/2021 05:01:56 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|██▌       | 504/1944 [15:25<9:39:50, 24.16s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 130500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|███▊      | 749/1944 [15:31<00:27, 43.03it/s]  07/17/2021 05:02:09 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  39%|███▊      | 749/1944 [15:50<00:27, 43.03it/s]07/17/2021 05:02:36 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "07/17/2021 05:09:28 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|███▉      | 755/1944 [23:00<7:29:31, 22.68s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 130750 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  51%|█████     | 995/1944 [23:06<00:22, 42.96it/s]  07/17/2021 05:09:44 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  51%|█████     | 995/1944 [23:20<00:22, 42.96it/s]07/17/2021 05:10:10 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "07/17/2021 05:17:02 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|█████▏    | 1004/1944 [30:32<5:12:18, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 131000 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  64%|██████▍   | 1249/1944 [30:37<00:16, 43.11it/s]  07/17/2021 05:17:15 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  64%|██████▍   | 1249/1944 [30:50<00:16, 43.11it/s]07/17/2021 05:17:42 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "07/17/2021 05:24:49 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|██████▍   | 1254/1944 [38:21<4:50:29, 25.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 131250 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|███████▋  | 1499/1944 [38:27<00:10, 43.33it/s]  07/17/2021 05:25:05 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  77%|███████▋  | 1499/1944 [38:42<00:10, 43.33it/s]07/17/2021 05:25:31 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "07/17/2021 05:32:21 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|███████▋  | 1505/1944 [45:50<2:43:46, 22.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 131500 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|████████▉ | 1745/1944 [45:56<00:04, 43.31it/s]  07/17/2021 05:32:34 - INFO - __main__ -   Scaling attention values to 0-255 ...\n",
      "Evaluating:  90%|████████▉ | 1745/1944 [46:12<00:04, 43.31it/s]07/17/2021 05:33:00 - INFO - __main__ -   Processing to 2048 value representations through barlow_twins ...\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "07/17/2021 05:39:53 - INFO - __main__ -   Appending results to array/dataframe ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening layers/heads ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|█████████ | 1755/1944 [53:25<59:29, 18.88s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- eval to representation batch 131750 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████▉| 1940/1944 [53:29<00:00, 43.19it/s]07/17/2021 05:40:07 - INFO - __main__ -     Outputting Attention File representation_df_131944 to /rapids/notebooks/host/QA_attentions_pickled/representations\n",
      "Evaluating: 100%|██████████| 1944/1944 [1:01:43<00:00,  1.90s/it]\n",
      "07/17/2021 05:48:21 - INFO - __main__ -     Evaluation done in total 3703.045666 secs (0.028065 sec per example)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8a262ffbf045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"_{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path, output_attentions=True)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name_or_path, output_attentions=True)\n",
    "device = \"cuda:1\"\n",
    "model = model.to(device)\n",
    "# Evaluate\n",
    "processor = SquadV2Processor() if version_2_with_negative else SquadV1Processor()\n",
    "result = evaluate({'data_dir': \"\"}, model, tokenizer)\n",
    "\n",
    "result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
    "results.update(result)\n",
    "\n",
    "logger.info(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
