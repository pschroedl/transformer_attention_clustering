{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers\n",
    "!pip install wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    squad_convert_examples_to_features,\n",
    ")\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertConfig, BertTokenizer\n",
    "\n",
    "from transformers.data.metrics.squad_metrics import (\n",
    "    compute_predictions_log_probs,\n",
    "    compute_predictions_logits,\n",
    "    squad_evaluate,\n",
    ")\n",
    "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
    "from transformers.trainer_utils import is_main_process\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "\n",
    "model_name_or_path = 'models/bert/'\n",
    "# cache_dir=cache_dir = 'models/cache'\n",
    "\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "max_query_length = 10\n",
    "threads = 12\n",
    "n_gpu = 1\n",
    "\n",
    "input_dir = \"./data/squad\"\n",
    "output_dir = \"./models/bert/\"\n",
    "model_type=\"bert\"\n",
    "# evaluate = True\n",
    "train_file = \"train-v2.0.json\"\n",
    "version_2_with_negative=True\n",
    "per_gpu_eval_batch_size=16\n",
    "\n",
    "\n",
    "n_best_size=20\n",
    "max_answer_length=30\n",
    "do_lower_case=True\n",
    "verbose_logging=True\n",
    "null_score_diff_threshold=0.0\n",
    "\n",
    "global_attention = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1\n",
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=0,1\n",
    "%set_env CUDA_DEVICE_ORDER=PCI_BUS_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
    "\n",
    "#     if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "#         os.makedirs(args.output_dir)\n",
    "\n",
    "    eval_batch_size = 1 * max(1, n_gpu)\n",
    "\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(dataset))\n",
    "    logger.info(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "    all_attentions = []\n",
    "    start_time = timeit.default_timer()\n",
    "    attn_count = 0\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2],\n",
    "            }\n",
    "\n",
    "            if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "\n",
    "            feature_indices = batch[3]\n",
    "\n",
    "            # XLNet and XLM use more arguments for their predictions\n",
    "#             if args.model_type in [\"xlnet\", \"xlm\"]:\n",
    "#                 inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n",
    "#                 # for lang_id-sensitive xlm models\n",
    "#                 if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
    "#                     inputs.update(\n",
    "#                         {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
    "#                     )\n",
    "#             inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "#             outputs_attention = model(**inputs, output_attentions=True)\n",
    "            \n",
    "        for i, feature_index in enumerate(feature_indices):\n",
    "            eval_feature = features[feature_index.item()]\n",
    "            unique_id = int(eval_feature.unique_id)\n",
    "\n",
    "#             output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
    "\n",
    "#             # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n",
    "#             # models only use two.\n",
    "#             if len(output) >= 5:\n",
    "#                 start_logits = output[0]\n",
    "#                 start_top_index = output[1]\n",
    "#                 end_logits = output[2]\n",
    "#                 end_top_index = output[3]\n",
    "#                 cls_logits = output[4]\n",
    "\n",
    "#                 result = SquadResult(\n",
    "#                     unique_id,\n",
    "#                     start_logits,\n",
    "#                     end_logits,\n",
    "#                     start_top_index=start_top_index,\n",
    "#                     end_top_index=end_top_index,\n",
    "#                     cls_logits=cls_logits,\n",
    "#                 )\n",
    "\n",
    "#             else:\n",
    "\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            attentions = get_layers(outputs.attentions)\n",
    "#             print(\"attentions:\")\n",
    "#             print(attentions)\n",
    "#             print(outputs.keys())\n",
    "            result = SquadResult(unique_id, start_logits, end_logits)\n",
    "            all_results.append(result)\n",
    "            append_list_as_row('QA_bert_attentions.csv', attentions)\n",
    "            all_attentions.append(attentions)\n",
    "            attn_count += 1\n",
    "            if attn_count % 100 == 0:\n",
    "                logger.info(\"  Outputting Attention File %s eval_attentions %i\", output_dir, attn_count)\n",
    "                torch.save(all_attentions, \"QA_attentions_pickled/eval_attentions_\" +str(attn_count)+\".bin\")\n",
    "                all_attentions = []\n",
    "\n",
    "    evalTime = timeit.default_timer() - start_time\n",
    "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
    "#     global_attention = all_attentions\n",
    "#     logger.info(\"  Outputting Attention File %s eval_attentions.bin\", output_dir)\n",
    "#     torch.save(all_attentions, os.path.join(output_dir, \"eval_attentions.bin\"))\n",
    "    \n",
    "\n",
    "    \n",
    "#     with open(os.path.join(output_dir, \"eval_attentions.pkl\"), 'wb') as attention_file:\n",
    "#       pickle.dump(all_attentions, attention_file)\n",
    "\n",
    "    # Compute predictions\n",
    "    output_prediction_file = os.path.join(output_dir, \"predictions_{}.json\".format(prefix))\n",
    "    output_nbest_file = os.path.join(output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "\n",
    "    if version_2_with_negative:\n",
    "        output_null_log_odds_file = os.path.join(output_dir, \"null_odds_{}.json\".format(prefix))\n",
    "    else:\n",
    "        output_null_log_odds_file = None\n",
    "\n",
    "#     # XLNet and XLM use a more complex post-processing procedure\n",
    "#     if args.model_type in [\"xlnet\", \"xlm\"]:\n",
    "#         start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n",
    "#         end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n",
    "\n",
    "#         predictions = compute_predictions_log_probs(\n",
    "#             examples,\n",
    "#             features,\n",
    "#             all_results,\n",
    "#             args.n_best_size,\n",
    "#             args.max_answer_length,\n",
    "#             output_prediction_file,\n",
    "#             output_nbest_file,\n",
    "#             output_null_log_odds_file,\n",
    "#             start_n_top,\n",
    "#             end_n_top,\n",
    "#             args.version_2_with_negative,\n",
    "#             tokenizer,\n",
    "#             args.verbose_logging,\n",
    "#         )\n",
    "#     else:\n",
    "#     predictions = compute_predictions_logits(\n",
    "#         examples,\n",
    "#         features,\n",
    "#         all_results,\n",
    "#         n_best_size,\n",
    "#         max_answer_length,\n",
    "#         do_lower_case,\n",
    "#         output_prediction_file,\n",
    "#         output_nbest_file,\n",
    "#         output_null_log_odds_file,\n",
    "#         verbose_logging,\n",
    "#         version_2_with_negative,\n",
    "#         null_score_diff_threshold,\n",
    "#         tokenizer,\n",
    "#     )\n",
    "\n",
    "#     # Compute the F1 and exact scores.\n",
    "#     results = squad_evaluate(examples, predictions)\n",
    "#     return results\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(attention, num_layers=12, num_heads=12):\n",
    "  layers = np.ndarray((num_heads,num_layers), np.ndarray)\n",
    "  for i, layer in enumerate(attention):\n",
    "    layer = layer.detach().cpu().numpy()[0]\n",
    "    for j, head in enumerate(layer):\n",
    "      layers[i,j] = head\n",
    "  return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from csv import writer\n",
    "def append_list_as_row(file_name, list_of_elem):\n",
    "    # Open file in append mode\n",
    "    with open(file_name, 'a+', newline='') as write_obj:\n",
    "        # Create a writer object from csv module\n",
    "        csv_writer = writer(write_obj)\n",
    "        # Add contents of list as last row in the csv file\n",
    "        csv_writer.writerow(list_of_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
    "    # Load data features from cache or dataset file\n",
    "#     input_dir = data_dir else \".\"\n",
    "\n",
    "    cached_features_file = os.path.join(\n",
    "        input_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            \"train\",\n",
    "            list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(max_seq_length),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    logger.info(\"cached features file: %s\", cached_features_file)\n",
    "#     Init features and dataset from cache if it exists\n",
    "    if os.path.exists(cached_features_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features_and_dataset = torch.load(cached_features_file)\n",
    "        features, dataset, examples = (\n",
    "            features_and_dataset[\"features\"],\n",
    "            features_and_dataset[\"dataset\"],\n",
    "            features_and_dataset[\"examples\"],\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
    "\n",
    "        examples = processor.get_train_examples(input_dir, filename=train_file)\n",
    "\n",
    "        logger.info(\"Got features from dataset file at %s\", input_dir)\n",
    "\n",
    "        features, dataset = squad_convert_examples_to_features(\n",
    "            examples=examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=max_seq_length,\n",
    "            doc_stride=doc_stride,\n",
    "            max_query_length=max_query_length,\n",
    "            is_training=False,\n",
    "            return_dataset=\"pt\",\n",
    "            threads=threads,\n",
    "        )\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
    "\n",
    "    if output_examples:\n",
    "        return dataset, examples, features\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.warning(\n",
    "    \"device: %s, n_gpu: %s\",\n",
    "    device,\n",
    "    n_gpu\n",
    ")\n",
    "# Set the verbosity to info of the Transformers logger (on main process only):\n",
    "# if is_main_process(args.local_rank):\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "config = BertConfig.from_pretrained(model_name_or_path, output_attentions=True) # no config_path?\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    do_lower_case=True,\n",
    "    cache_dir=input_dir,\n",
    "    use_fast=False,  # SquadDataset is not compatible with Fast tokenizers which have a smarter overflow handeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path, output_attentions=True)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name_or_path, output_attentions=True)\n",
    "device = \"cuda:1\"\n",
    "model = model.to(device)\n",
    "# Evaluate\n",
    "processor = SquadV2Processor() if version_2_with_negative else SquadV1Processor()\n",
    "result = evaluate({'data_dir': \"\"}, model, tokenizer)\n",
    "\n",
    "result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
    "results.update(result)\n",
    "\n",
    "logger.info(\"Results: {}\".format(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " attentions = torch.load(os.path.join(output_dir, \"eval_attentions.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " 'attentions',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
